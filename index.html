<!doctype html>



  


<html class="theme-next pisces use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="Ej91TiJi7CrymEW4RNSdR1TQpfxw1DzXHW3faQuzzZQ" />










  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hadoop, MapReduce, HDFS, Spark" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="跟我一起学Hadoop 从零开始由浅入深">
<meta property="og:type" content="website">
<meta property="og:title" content="Big data及Hadoop分享">
<meta property="og:url" content="http://www.hellospark.me/index.html">
<meta property="og:site_name" content="Big data及Hadoop分享">
<meta property="og:description" content="跟我一起学Hadoop 从零开始由浅入深">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Big data及Hadoop分享">
<meta name="twitter:description" content="跟我一起学Hadoop 从零开始由浅入深">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"always"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>

  <title> Big data及Hadoop分享 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-77178944-1', 'auto');
  ga('send', 'pageview');
</script>


  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?dc4072144e750437800dbc3aef534814";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>








  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Big data及Hadoop分享</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">HDFS MapReduce Spark</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/06/23/scalar-notes/" itemprop="url">
                  Scala 语法杂记
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-06-23T23:14:20+08:00" content="2016-06-23">
              2016-06-23
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/06/23/scalar-notes/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/06/23/scalar-notes/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Scala-语法杂记"><a href="#Scala-语法杂记" class="headerlink" title="Scala 语法杂记"></a>Scala 语法杂记</h2><h3 id="Define-a-function"><a href="#Define-a-function" class="headerlink" title="Define a function"></a>Define a function</h3><pre><code>def isHeader(line: String): Boolean = {
    line.contains(&quot;id_1&quot;)
}
</code></pre><ul>
<li>Argument is line with type String</li>
<li>Return type is “Boolean”.  </li>
<li>Scala is able to infer the return type itself. So it’s not neccessary to specify the return type. PS: it’s the best practice to specify the return type.</li>
</ul>
<h3 id="Define-an-anonymous-function"><a href="#Define-an-anonymous-function" class="headerlink" title="Define an anonymous function"></a>Define an anonymous function</h3><ul>
<li>Use underscore(_) as aurgument to lambda<ul>
<li><code>head.filter(!isHeader(_)).foreach(println)</code></li>
</ul>
</li>
<li>Pass the argument explictly to lambda    <ul>
<li><code>head.filter(x =&gt; !isHeader(x)).foreach(println)</code></li>
</ul>
</li>
</ul>
<h3 id="pass-the-function-as-parameter"><a href="#pass-the-function-as-parameter" class="headerlink" title="pass the function as parameter"></a>pass the function as parameter</h3><p><code>head.foreach(println)</code><br>println is a function</p>
<h3 id="define-variable"><a href="#define-variable" class="headerlink" title="define variable"></a>define variable</h3><ul>
<li>Define a const varialbe<ul>
<li><code>val rawblocks = sc.textFile(&quot;/mydata&quot;)</code></li>
</ul>
</li>
<li>Define a non-const varialbe<ul>
<li><code>var rawblocks = sc.textFiel(&quot;/mydata&quot;)</code></li>
</ul>
</li>
</ul>
<h3 id="apply-method"><a href="#apply-method" class="headerlink" title="apply method"></a>apply method</h3><p>Scala class can define apply() method to make a object acting like a function.  Just like overlaod the ()operator in C++.<br>Following 2 lines are the same:</p>
<pre><code>head(5)
head.apply(5)
</code></pre><h3 id="tuple"><a href="#tuple" class="headerlink" title="tuple"></a>tuple</h3><pre><code>val tup=(id1, id2, scores, matched)
tup._1
tup._2
tup.productElement(0)
tup.productArity
</code></pre><ul>
<li>_1 is the first element, _2 the scecond one</li>
<li>productElement(0) returns the first</li>
<li>productArity returns the size</li>
<li>Cannot access the element by tup(1)</li>
</ul>
<h3 id="case-classes"><a href="#case-classes" class="headerlink" title="case classes."></a>case classes.</h3><p>A case class is a simple type of immutable class that comes with implementations of all of the basic Java class methods, like toString, equals, and hashCode, which makes them very easy to use. Let’s declare a case class:</p>
<pre><code>scala&gt; case class MatchData(id1: Int, id2: Int, scores: Array[Double], matched: Boolean)
defined class MatchData

scala&gt; def parse(line: String) = {
 | val pieces = line.split(&quot;,&quot;)
 | val id1 = pieces(0).toInt
 | val id2 = pieces(1).toInt
 | val scores = pieces.slice(2,11).map(toDouble)
 | val matched = pieces(11).toBoolean
 | MatchData(id1, id2, scores, matched)
 | }
parse: (line: String)MatchData

scala&gt; val md = parse(line)
md: MatchData = MatchData(36950,42116,[D@20361260,true)

scala&gt; md.scores
res26: Array[Double] = Array(1.0, NaN, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)

scala&gt; md.id1
res27: Int = 36950

scala&gt; 
</code></pre><ul>
<li>No need to specify keyword “new” when creating an instance</li>
<li>Class “MatchData” comes with a build-in “toString” implementation that works great for each member except the ‘scores’ Array.</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/06/03/MapReduce-code-go-through/" itemprop="url">
                  MapReduce 代码走读
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-06-03T08:49:04+08:00" content="2016-06-03">
              2016-06-03
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/06/03/MapReduce-code-go-through/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/06/03/MapReduce-code-go-through/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>通过代码学习可能比看书更加适合广大攻城狮/程序猿们. 让我们来通过一个最常见的例子”WordCount”来看看,MapReduce常用的几个类如何使用以及之间的关系.</p>
<h2 id="WordCount"><a href="#WordCount" class="headerlink" title="WordCount"></a>WordCount</h2><p>这个WordCount是Hadoop官方文档里面给的一个简单的MapReduce范例代码. 顾名思义,它是用来计算文件里面每个单词出现的次数. 下面是他的代码,然后让我们来一起 <del>reading the F**king source code</del> 学习学习.</p>
<pre><code>package lyndon.my_mr;

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.io.VIntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.fs.Path;


public class MyWordCounter
{
    public static class MyWordCounterMapper extends Mapper&lt;LongWritable, Text, Text, VIntWritable&gt;
    {
        private final static VIntWritable one = new VIntWritable(1);
        private Text word = new Text();

        public void map(LongWritable key, Text value, Context context)
                throws IOException, InterruptedException
        {
            StringTokenizer itr = new StringTokenizer(value.toString());
            while(itr.hasMoreTokens())
            {
                word.set(itr.nextToken());
                context.write(word, one);
            }
        }
    }

    public static class MyWordCounterReducer extends Reducer&lt;Text, VIntWritable, Text, VIntWritable&gt;
    {
        private VIntWritable result = new VIntWritable();
        public void reduce(Text key, Iterable&lt;VIntWritable&gt; values, Context context) 
                throws IOException, InterruptedException
        {
            int sum = 0;
            for(VIntWritable val : values)
            {
                sum += val.get();
            }
            result.set(sum);
            context.write(key, result);
        }
    }

    public static void main(String args[]) throws Exception
    {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, &quot;my word counter&quot;);
        job.setJarByClass(MyWordCounter.class);
        job.setMapperClass(MyWordCounterMapper.class);
        job.setReducerClass(MyWordCounterReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(VIntWritable.class);

        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
</code></pre><p>这段代码仔细看分为三部分:</p>
<ol>
<li>MyWordCounterMapper: Mapper类的实现</li>
<li>MyWordCounterReducer: Reducer类的实现</li>
<li>main()方法: 构造Job类的instance</li>
</ol>
<h2 id="Job类"><a href="#Job类" class="headerlink" title="Job类"></a>Job类</h2><p>在工作中,我们工程师的工作应该是集中于开发Mapper和Reducer. 但是处于学习和理解MapReduce的目的,我们先研究下Job.<br>通过上面的代码,我们可以看到Job的对象被设置了如下几项:</p>
<ol>
<li>Configuration 对象</li>
<li>Mapper类</li>
<li>Reducer类</li>
<li>Output Key/Value 类型</li>
<li>输入输出Path</li>
</ol>
<p>job.waitForCompletion()方法会submit job,然后等它执行结束.  那么问题来了,submit()方法干了啥?</p>
<p>###Job类中submit()的实现:</p>
<pre><code>public void submit() 
       throws IOException, InterruptedException, ClassNotFoundException {
  ensureState(JobState.DEFINE);
  setUseNewAPI();
  connect();
  final JobSubmitter submitter = 
      getJobSubmitter(cluster.getFileSystem(), cluster.getClient());
  status = ugi.doAs(new PrivilegedExceptionAction&lt;JobStatus&gt;() {
    public JobStatus run() throws IOException, InterruptedException, 
    ClassNotFoundException {
      return submitter.submitJobInternal(Job.this, cluster);
    }
  });
  state = JobState.RUNNING;
  LOG.info(&quot;The url to track the job: &quot; + getTrackingURL());
 }
</code></pre><p>由此可见, submit方法主要是构造了一个JobSubmitter类的instance然后调用 submitter.submitJobInternal()方法.  </p>
<p>###JobSubmitter类中submitJobInternal()的实现：</p>
<pre><code>JobStatus submitJobInternal(Job job, Cluster cluster) 
throws ClassNotFoundException, InterruptedException, IOException {

  //validate the jobs output specs 
  checkSpecs(job);

  Configuration conf = job.getConfiguration();
  addMRFrameworkToDistributedCache(conf);

  Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);
  //configure the command line options correctly on the submitting dfs
  InetAddress ip = InetAddress.getLocalHost();
  if (ip != null) {
    submitHostAddress = ip.getHostAddress();
    submitHostName = ip.getHostName();
    conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);
    conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);
  }
  JobID jobId = submitClient.getNewJobID();
  job.setJobID(jobId);
  Path submitJobDir = new Path(jobStagingArea, jobId.toString());
  JobStatus status = null;
  try {
    conf.set(MRJobConfig.USER_NAME,
        UserGroupInformation.getCurrentUser().getShortUserName());
    conf.set(&quot;hadoop.http.filter.initializers&quot;, 
        &quot;org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer&quot;);
    conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());
    LOG.debug(&quot;Configuring job &quot; + jobId + &quot; with &quot; + submitJobDir 
        + &quot; as the submit dir&quot;);
    // get delegation token for the dir
    TokenCache.obtainTokensForNamenodes(job.getCredentials(),
        new Path[] { submitJobDir }, conf);

    populateTokenCache(conf, job.getCredentials());

    // generate a secret to authenticate shuffle transfers
    if (TokenCache.getShuffleSecretKey(job.getCredentials()) == null) {
      KeyGenerator keyGen;
      try {
        keyGen = KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);
        keyGen.init(SHUFFLE_KEY_LENGTH);
      } catch (NoSuchAlgorithmException e) {
        throw new IOException(&quot;Error generating shuffle secret key&quot;, e);
      }
      SecretKey shuffleKey = keyGen.generateKey();
      TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),
          job.getCredentials());
    }
    if (CryptoUtils.isEncryptedSpillEnabled(conf)) {
      conf.setInt(MRJobConfig.MR_AM_MAX_ATTEMPTS, 1);
      LOG.warn(&quot;Max job attempts set to 1 since encrypted intermediate&quot; +
              &quot;data spill is enabled&quot;);
    }

    copyAndConfigureFiles(job, submitJobDir);

    Path submitJobFile = JobSubmissionFiles.getJobConfPath(submitJobDir);

    // Create the splits for the job
    LOG.debug(&quot;Creating splits at &quot; + jtFs.makeQualified(submitJobDir));
    int maps = writeSplits(job, submitJobDir);
    conf.setInt(MRJobConfig.NUM_MAPS, maps);
    LOG.info(&quot;number of splits:&quot; + maps);

    // write &quot;queue admins of the queue to which job is being submitted&quot;
    // to job file.
    String queue = conf.get(MRJobConfig.QUEUE_NAME,
        JobConf.DEFAULT_QUEUE_NAME);
    AccessControlList acl = submitClient.getQueueAdmins(queue);
    conf.set(toFullPropertyName(queue,
        QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());

    // removing jobtoken referrals before copying the jobconf to HDFS
    // as the tasks don&apos;t need this setting, actually they may break
    // because of it if present as the referral will point to a
    // different job.
    TokenCache.cleanUpTokenReferral(conf);

    if (conf.getBoolean(
        MRJobConfig.JOB_TOKEN_TRACKING_IDS_ENABLED,
        MRJobConfig.DEFAULT_JOB_TOKEN_TRACKING_IDS_ENABLED)) {
      // Add HDFS tracking ids
      ArrayList&lt;String&gt; trackingIds = new ArrayList&lt;String&gt;();
      for (Token&lt;? extends TokenIdentifier&gt; t :
          job.getCredentials().getAllTokens()) {
        trackingIds.add(t.decodeIdentifier().getTrackingId());
      }
      conf.setStrings(MRJobConfig.JOB_TOKEN_TRACKING_IDS,
          trackingIds.toArray(new String[trackingIds.size()]));
    }

    // Set reservation info if it exists
    ReservationId reservationId = job.getReservationId();
    if (reservationId != null) {
      conf.set(MRJobConfig.RESERVATION_ID, reservationId.toString());
    }

    // Write job file to submit dir
    writeConf(conf, submitJobFile);

    //
    // Now, actually submit the job (using the submit name)
    //
    printTokens(jobId, job.getCredentials());
    status = submitClient.submitJob(
        jobId, submitJobDir.toString(), job.getCredentials());
    if (status != null) {
      return status;
    } else {
      throw new IOException(&quot;Could not launch job&quot;);
    }
  } finally {
    if (status == null) {
      LOG.info(&quot;Cleaning up the staging area &quot; + submitJobDir);
      if (jtFs != null &amp;&amp; submitJobDir != null)
        jtFs.delete(submitJobDir, true);

    }
  }
}
</code></pre><p>这个方法的主要任务是：</p>
<ol>
<li>检查Input/Output的Spec</li>
<li>为Job计算InputSplits</li>
<li>把Jar文件和conf文件拷贝到HDFS的map-reduce系统目录</li>
<li>向JobTracker提交job请求，然后监视job状态</li>
</ol>
<p>2#计算InputSplits是由FileInputFormat类中的getSplits()方法实现。下面是计算getSplits()方法的一段代码。 </p>
<pre><code>if (isSplitable(job, path)) 
{
      long blockSize = file.getBlockSize();
      long splitSize = computeSplitSize(blockSize, minSize, maxSize);

      long bytesRemaining = length;
      while (((double) bytesRemaining)/splitSize &gt; SPLIT_SLOP) {
        int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);
        splits.add(makeSplit(path, length-bytesRemaining, splitSize,
                    blkLocations[blkIndex].getHosts(),
                    blkLocations[blkIndex].getCachedHosts()));
        bytesRemaining -= splitSize;
      }

      if (bytesRemaining != 0) {
        int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);
        splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining,
                   blkLocations[blkIndex].getHosts(),
                   blkLocations[blkIndex].getCachedHosts()));
      }
    }
</code></pre><p>3#JobTracker在本地文件系统有个系统目录，名字就叫做’jobTracker’. 每一个正在运行的job，都有两个文件在这个目录下：</p>
<ul>
<li>jobTracker/<jobid>.xml</jobid></li>
<li>jobTracker/<jobid>.jar </jobid></li>
</ul>
<p>4#提交job请求，是由ClientProtocol类对象的submitJob()方法来实现的。 ClientProtocol是JobClient和中央JobTracker通信的协议。</p>
<h1 id="Mapper-类"><a href="#Mapper-类" class="headerlink" title="Mapper 类"></a>Mapper 类</h1><p>Mapper的工作流程有run（）方法控制，用户可以override这个方法来定制特殊的workflow.</p>
<pre><code>/**
 * Expert users can override this method for more complete control over the
 * execution of the Mapper.
 * @param context
 * @throws IOException
 */
public void run(Context context) throws IOException, InterruptedException {
  setup(context);
  try {
    while (context.nextKeyValue()) {
      map(context.getCurrentKey(), context.getCurrentValue(), context);
    }
  } finally {
    cleanup(context);
  }
}
</code></pre><p>Run（）方法先运行setup（）然后调用map（）方法处理每个context，最后执行cleanup（）<br>一般我们只需要实现自己的map（）方法，但是上面出现的四个方法，我们都可以override，取决于自己项目的需要。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/05/11/YARN/" itemprop="url">
                  YARN详细介绍
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-05-11T22:49:17+08:00" content="2016-05-11">
              2016-05-11
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/05/11/YARN/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/05/11/YARN/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>Yarn本来只是MapReduce的一部分,后来变成了Hadoop一个独立的项目. 让我们带着如下问题一起来探讨下Yarn:</p>
<ul>
<li>Yarn相比于其前身MapReduce 1到底有什么改进?</li>
<li>Yarn包含哪些组件?</li>
<li>一个Application到底是如何再Yarn中运行的.</li>
</ul>
<h1 id="YARN的前身-MapReduce-1"><a href="#YARN的前身-MapReduce-1" class="headerlink" title="YARN的前身 - MapReduce 1"></a>YARN的前身 - MapReduce 1</h1><p>在YARN之前,Hadoop的MapReduce系统由一个JobTracker和位于各个节点上的TaskTracker组成. 在介绍完Yarn后,我们一起比较下这两个架构.<br><img src="http://hortonworks.com/wp-content/uploads/2012/08/MRArch.png" alt="link"></p>
<p>JobTracker负责:</p>
<ul>
<li>资源管理</li>
<li>跟踪资源消耗/可用资源</li>
<li>Job生命周期管理(调度Job的每个task,跟踪状态,提高灾难恢复等)</li>
</ul>
<p>TaskTracker的职责非常简单: </p>
<ul>
<li>根据JobTracker的指示启动/销毁Task</li>
<li>周期性的想JobTracker报告Task状态</li>
</ul>
<h1 id="YARN介绍"><a href="#YARN介绍" class="headerlink" title="YARN介绍"></a>YARN介绍</h1><p>YARN把JobTracker的拆分成两个组件: 一个全局的ResourceManager和每个application一个的ApplicationMaster.</p>
<p><strong>ResourceMnager</strong>是所有applications的资源仲裁终极机构. ResourceManager有一个pluggable的scheduler. schedualer负责根据你capacity/queque的配置来为各个application分配资源. </p>
<p>scheduler是一个纯粹的调度器,它不会监视application的状态,也不为application提供任何灾难恢复. Scheduler基于application的资源请求执行自己的调度任务, 它调度资源的时候基于Container的概念.  Container由各种资源组成,比如memory,cpu,disk,network等等</p>
<p><strong>ApplicationMaster</strong>和schedualer磋商资源,然后和NodeManager一起启动Container. ApplicationMaster监视每个container的状态. 站在Hadoop系统的角度上, ApplicationMaster也是一个普通的container.</p>
<p><strong>NodeManager</strong>运行在每个node上. 它负责启动application的container.然后向ResourceManager报告资源的使用状态.</p>
<h3 id="Yarn的架构如下"><a href="#Yarn的架构如下" class="headerlink" title="Yarn的架构如下:"></a>Yarn的架构如下:</h3><img src="/images/YARNArch.png">
<h1 id="YARN的优点"><a href="#YARN的优点" class="headerlink" title="YARN的优点"></a>YARN的优点</h1><ul>
<li>Scalability: <ul>
<li>MapReduce 1 最多可支持4000个节点的集群.因为JobTracker负责的职责太多而成为瓶颈</li>
<li>Yarn 可以支持10000个节点,并行100000个task.</li>
</ul>
</li>
<li>Availability: <ul>
<li>MapReduce 1 的JobTracker的状态变化非常迅速(想象下每个Task过几秒都会想它报告状态). 这使得JotTracker很难实现HA(高可用性). 通常HA都是通过备份当前系统的状态然后当系统失败备用系统用备份的状态来继续工作.</li>
<li>Yarn的ResourceManager职责很简单,很容易实现HA</li>
</ul>
</li>
<li>Utilization: <ul>
<li>MapReduce 1中,每个TaskTracker都会配置好固定个数的Slots. 这些slots会被配置成Map slots和Reduce slots. Map slot只能用来运行Map Task, Reduce slot只能运行Reduce Task. 这样资源利用率很低.</li>
<li>Yarn中没有slot的概念. NodeManager管理一个资源池,按需启动Container.</li>
</ul>
</li>
<li>Multitenancy: <ul>
<li>MapReduce 1只能运行MapReduce应用.</li>
<li>Yarn最大的好处之一就是职称很多其他类型的分布式Application. 如下图:<img src="/images/hddg_0401.png">
</li>
</ul>
</li>
</ul>
<h1 id="YARN如何运行application"><a href="#YARN如何运行application" class="headerlink" title="YARN如何运行application"></a>YARN如何运行application</h1><img src="/images/yarnflow.png">
<h3 id="如上图所示-YARN中一个application的运行流程如下"><a href="#如上图所示-YARN中一个application的运行流程如下" class="headerlink" title="如上图所示, YARN中一个application的运行流程如下:"></a>如上图所示, YARN中一个application的运行流程如下:</h3><ol>
<li>Client提交一个application，以及必要的specification来启动ApplicationMaster</li>
<li>ResourceManager敲定一个用来启动ApplicationMaster的container。 然后启动ApplicationMaster</li>
<li>ApplicationMaster启动时会向ResourceManager注册。 注册后，client可以向ResourceManager查询ApplicationMaster的详细信息，并且client可以直接和ApplicationMaster通信。</li>
<li>ApplicationMaster通过resourcerequest 协议来敲定更多的Container资源</li>
<li>ApplicationMaster向NodeManager提供详细的信息来启动Container。 之后Container可以和ApplicationMaster通信</li>
<li>application会在Container里面执行，并根据applicaion-specific的协议来向ApplicationMaster报告状态</li>
<li>在appliation运行期间，client直接通过application-specific协议来和ApplicationMaster通信去获取运行状态以及进展</li>
<li>application结束后，ApplicationMater会向ResourceManager注销然后释放资源。</li>
</ol>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="http://hortonworks.com/blog/apache-hadoop-yarn-background-and-an-overview/" target="_blank" rel="external">http://hortonworks.com/blog/apache-hadoop-yarn-background-and-an-overview/</a></li>
<li><a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="external">http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html</a></li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/05/07/HDFS-Architecture/" itemprop="url">
                  HDFS架构详解(天坑待续)
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-05-07T23:03:43+08:00" content="2016-05-07">
              2016-05-07
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/05/07/HDFS-Architecture/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/05/07/HDFS-Architecture/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>分布式文件系统（HDFS）用来存储非常大的数据集。HDFS保证数据的可靠性的同时能够为应用程序提供高的吞吐量。在一个大的集群里面，数千个节点各自执行存储和运算任务。通过分布式存储和计算，资源可以用经济性的设备按照需求来调整。</p>
<h1 id="HDFS介绍-待续"><a href="#HDFS介绍-待续" class="headerlink" title="HDFS介绍(待续)"></a>HDFS介绍(待续)</h1><p>Hadoop提供了HDFS和MapReduce框架来分析和转化非常大的数据集。HDFS的使用起来很像Unix的文件系统。提供的命令行接口基本和Unix一致， 比如 hdfs dfs -ls / -rm / -cat /-mv.   但是为了提高应用程序的性能，<br>HDFS并不遵守Unix的标准。</p>
<p>Hadoop的主要特征就是：</p>
<ul>
<li>把数据分块</li>
<li>在很多（以千计）的节点间运算</li>
<li>在靠近数据的地方并行的运算应用程序</li>
</ul>
<p>Hadoop集群用经济型的服务器来增加计算，存储，IO带宽的规模。HDFS把应用数据和元数据分开来存储。<br>HDFS把元数据（metadata）存放在一个专门的服务器上，叫做Namenode。应用数据被存储在其他重点服务器上，他们叫做Datanode。<br>所有的服务器都被连接起来，用TCP/IP协议来交流。为了数据可靠性，应用数据都会被复制到不同的多个Datanode上。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/05/05/Hadoop-IO-Data-Integrity-part1/" itemprop="url">
                  HDFS数据完整性介绍
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-05-05T06:47:38+08:00" content="2016-05-05">
              2016-05-05
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/05/05/Hadoop-IO-Data-Integrity-part1/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/05/05/Hadoop-IO-Data-Integrity-part1/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Data-integrity"><a href="#Data-integrity" class="headerlink" title="Data integrity"></a>Data integrity</h1><p>Because every I/O operation on the disk or network carries with it a small chance of introducing errors into the data that it is reading or writing, when the volumes of data flowing through the system are as large as the ones Hadoop is capable of handling, the chance of data corruption occurring is high.</p>
<p>The usual way of detecting corrupted data is by computing a checksum for the data when it first enters the system. </p>
<p>CRC-32 is used for checksumming in Hadoop’s ChecksumFileSystem, while HDFS uses a more efficient variant called CRC-32C.</p>
<h2 id="Data-integrity-in-HDFS"><a href="#Data-integrity-in-HDFS" class="headerlink" title="Data integrity in HDFS"></a>Data integrity in HDFS</h2><p>A separate checksum is created for every dfs.bytes-per-checksum bytes of data. The default is 512 bytes, and because a CRC-32C checksum is 4 bytes long, the storage overhead is less than 1%.</p>
<h3 id="Write"><a href="#Write" class="headerlink" title="Write"></a>Write</h3><p>Datanodes are responsible for verifying the data they receive before storing the data and its checksum. </p>
<p>A client writing data sends it to a pipeline of datanodes and the last datanode in the pipeline verifies the checksum. If the datanode detects an error, the client receives a subclass of IOException, which it should handle in an application-specific manner (for example, by retrying the operation).</p>
<h3 id="Read-and-Checksum-log"><a href="#Read-and-Checksum-log" class="headerlink" title="Read and Checksum log"></a>Read and Checksum log</h3><p>When clients read data from datanodes, they verify checksums as well, comparing them with the ones stored at the datanodes. Each datanode keeps a persistent log of checksum verifications, so it knows the last time each of its blocks was verified. When a client successfully verifies a block, it tells the datanode, which updates its log. Keeping statistics such as these is valuable in detecting bad disks.</p>
<h3 id="DataBlockScanner"><a href="#DataBlockScanner" class="headerlink" title="DataBlockScanner"></a>DataBlockScanner</h3><p>Each datanode runs a DataBlockScanner in a background thread that periodically verifies all the blocks stored on the datanode.</p>
<h3 id="Heal"><a href="#Heal" class="headerlink" title="Heal"></a>Heal</h3><p>It can “heal” corrupted blocks by copying one of the good replicas to produce a new, uncorrupt replica.</p>
<p>If a client detects an error when reading a block, it reports the bad block and the datanode it was trying to read from to the namenode before throwing a ChecksumException. The namenode marks the block replica as corrupt so it doesn’t direct any more clients  to it or try to copy this replica to another datanode.</p>
<h3 id="Disable-checksum"><a href="#Disable-checksum" class="headerlink" title="Disable checksum"></a>Disable checksum</h3><p>Disable verification of checksums by passing false to the setVerifyChecksum() method on FileSystem before using the open() method to read a file. The same effect is possible from the shell by using the -ignoreCrc option with the -get or the equivalent -copyToLocal command.</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/05/02/HDFS-Concepts-Blocks-NN-DN/" itemprop="url">
                  HDFS 基本概念介绍之Block
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-05-02T00:50:01+08:00" content="2016-05-02">
              2016-05-02
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/05/02/HDFS-Concepts-Blocks-NN-DN/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/05/02/HDFS-Concepts-Blocks-NN-DN/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="本篇讲下HDFS以下基本概念的第一项"><a href="#本篇讲下HDFS以下基本概念的第一项" class="headerlink" title="本篇讲下HDFS以下基本概念的第一项:"></a>本篇讲下HDFS以下基本概念的第一项:</h2><ul>
<li>Blocks</li>
<li>NameNode &amp; DataNode</li>
<li>Block Caching</li>
<li>Block Federation</li>
<li>HDFS Hight Availability</li>
<li>Failover and Fencing</li>
</ul>
<h1 id="Blocks"><a href="#Blocks" class="headerlink" title="Blocks"></a>Blocks</h1><p>基于单块磁盘的文件系统都有一个block size的概念. 每次读写的最小单位就是一个block. 一般情况下是512B.</p>
<p>HDFS也有一个block的概念, 但是它大多了(bigger than bigger). HDFS一个block的大小是128M. 但是不像普通的文件系统那样,小于一个block大小的文件也会占用一个block. </p>
<p><strong>Block大小为128MB的HDFS上面存储一个1MB大小的文件只会使用IMB磁盘空间,而不是128MB</strong></p>
<p>为什么HDFS的一个block要这么大?! 为了减少寻址(seek)的带价. 如果block够大,那么花在寻找block上的时间和传输真正数据的时间相比就可以忽略不计. </p>
<p>Block abstraction带来的好处:</p>
<ul>
<li>文件的大小可以远远超过某个单一磁盘的大小. 属于一个文件的所有blocks分散在不同磁盘的磁盘上,以提高集群的利用率.</li>
<li>使用Block还是file作为文件存储的单位可以简化HDFS的设计. Block的大小是固定的,没有metadata.</li>
<li>block replication很好的解决了容灾和高可用性. 每个block都会在不同的节点上存几个replica. 典型的情况是存3份replicas. 如果一个block不可用了, 其他block replicas可以继续服务客户端的读请求. 这对于客户端是透明的. 如果一个block彻底损坏了或者节点出错了,HDFS会自动在别的节点重新生成一个replica,来保证replica一直都是3份.</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/04/30/the-design-of-HDFS/" itemprop="url">
                  HDFS 的设计
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-04-30T22:07:56+08:00" content="2016-04-30">
              2016-04-30
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/04/30/the-design-of-HDFS/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/04/30/the-design-of-HDFS/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>HDFS是被设计用来存储大数据的文件系统.  它采用流式数据访问模式,运行在消费级硬件集群上. </p>
<h2 id="各个角度看HDFS"><a href="#各个角度看HDFS" class="headerlink" title="各个角度看HDFS"></a>各个角度看HDFS</h2><h3 id="大数据"><a href="#大数据" class="headerlink" title="大数据"></a>大数据</h3><p>多大才算大数据? 一般而言是指GB/TB级别的数据,几百兆也勉强算.  有很多hadoop集群运行着PB级别的数据.</p>
<h3 id="流式数据访问"><a href="#流式数据访问" class="headerlink" title="流式数据访问"></a>流式数据访问</h3><p>最有效的数据访问模式是:一次写入,多次读取.  HDFS就是建立在这样一个点子上.  一个数据集是被生成后后随着时间的推移会被用来做很多次数据分析.  每次分析基本都会去读这个数据集的大部分内容.  <strong>所以一次读取大量数据的效率要优先于一次读取第一小块数据的效率.</strong></p>
<h3 id="消费级硬件"><a href="#消费级硬件" class="headerlink" title="消费级硬件"></a>消费级硬件</h3><p>Hadoop不需要很贵的,高可用的硬件. 它运行在廉价集群上,这些硬件可以来自不同的品牌. 这样的集群中单点故障的发生频率很高. <strong>HDFS的设计使得在用户不知晓硬件故障的情况下能够不中断的工作.</strong></p>
<h3 id="低延时的数据访问"><a href="#低延时的数据访问" class="headerlink" title="低延时的数据访问"></a>低延时的数据访问</h3><p>需要低延时访问数据的程序(比如,&gt;10毫秒级别)不太适合使用HDFS.  HDFS为高吞吐量做了优化,这意味着会导致很高的延迟.  HBase可能更加适用于低延迟的程序.</p>
<h3 id="大量的小文件"><a href="#大量的小文件" class="headerlink" title="大量的小文件"></a>大量的小文件</h3><p>因为namenode把文件系统的metadata保存在memory里面, 一个HDFS的文件数量限制受制于namenode的memory有多大. 每一个文件,目录,block 都会占用150B内存. 100万个单block的文件大约需要300M内存. 所以存储百万级的文件是可行的的.  亿级别的文件数量超出了目前硬件的极限.</p>
<h3 id="多次写入-随意文件更改"><a href="#多次写入-随意文件更改" class="headerlink" title="多次写入,随意文件更改"></a>多次写入,随意文件更改</h3><p>HDFS里面的文件都会只有一个写入者.  写操作用用都是在文件的末尾, 也就是append-only fashion. 目前HDFS不支持并发写入以及更改操作.(将来有可能会支持)</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.png"
               alt="Lyndon Zhang" />
          <p class="site-author-name" itemprop="name">Lyndon Zhang</p>
          <p class="site-description motion-element" itemprop="description">跟我一起学Hadoop 从零开始由浅入深</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">7</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/1761587451/profile" target="_blank" title="weibo">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  weibo
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://github.com/dandanmylady" target="_blank" title="github">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  github
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-block">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://hadoop.apache.org" title="Hadoop" target="_blank">Hadoop</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://spark.apache.org" title="Spark" target="_blank">Spark</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://hellostack.me" title="HelloStack" target="_blank">HelloStack</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Lyndon Zhang</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>

        

<div class="busuanzi-count">

  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv"><i class="fa fa-user"></i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span></span>
  

  
    <span class="site-pv"><i class="fa fa-eye"></i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span>
  
  
</div>



        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.0.1"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"hellospark"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
  





  
  
  

  

  

</body>
</html>
